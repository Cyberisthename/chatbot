#!/usr/bin/env python3
"""
Direct conversion of trained HF model to GGUF format for Ollama.
This script uses transformers and other tools to convert the model.
"""

import os
import sys
import subprocess
from pathlib import Path

print("=" * 80)
print("üîÑ Converting Model to GGUF Format for Ollama")
print("=" * 80)

MODEL_PATH = "./jarvis-model"
OUTPUT_PATH = "./jarvis-ollama.gguf"
GGUF_DIR = "./gguf-exports"

# Ensure directories exist
Path(GGUF_DIR).mkdir(exist_ok=True, parents=True)

# Check if model exists
if not os.path.exists(MODEL_PATH):
    print(f"\n‚ùå Error: Model not found at {MODEL_PATH}")
    print("   Run train_and_export_gguf.py first to train the model")
    sys.exit(1)

print(f"\nüìÅ Model path: {MODEL_PATH}")
print(f"üìù Output path: {OUTPUT_PATH}")

# Step 1: Verify model files
print(f"\nüìã Verifying model files...")
required_files = ["pytorch_model.bin", "config.json", "tokenizer_config.json"]
for file in required_files:
    filepath = os.path.join(MODEL_PATH, file)
    if os.path.exists(filepath):
        size_mb = os.path.getsize(filepath) / (1024 * 1024)
        print(f"   ‚úÖ {file} ({size_mb:.1f}MB)")
    else:
        print(f"   ‚ö†Ô∏è {file} (missing, non-critical)")

# Step 2: Try conversion using available tools
print(f"\nüîß Attempting GGUF conversion...")

# Method 1: Try using llama.cpp's conversion script
print(f"\n  Method 1: Checking for llama.cpp conversion tools...")
try:
    # Look for the convert script in the venv
    from llama_cpp import Llama
    print("    ‚úÖ llama-cpp-python is available")
    
    # The conversion typically requires running the conversion script
    # which is part of the llama.cpp project
    conversion_tool = None
    
    # Check common locations
    venv_path = os.path.join(os.path.dirname(__file__), ".venv")
    for script_name in ["convert_hf_to_gguf.py", "convert.py"]:
        potential_path = os.path.join(venv_path, "bin", script_name)
        if os.path.exists(potential_path):
            conversion_tool = potential_path
            print(f"    Found: {conversion_tool}")
            break
    
    if conversion_tool:
        print(f"    Running conversion tool...")
        cmd = [
            sys.executable,
            conversion_tool,
            MODEL_PATH,
            "--outfile", OUTPUT_PATH,
            "--outtype", "q4_0",
        ]
        print(f"    $ {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=False, text=True)
        
        if result.returncode == 0:
            print(f"    ‚úÖ Conversion successful!")
    else:
        print(f"    ‚ö†Ô∏è Conversion tool not found in venv")
        
except ImportError:
    print(f"    ‚ÑπÔ∏è llama-cpp-python not found, skipping this method")

# Method 2: Create a quantized version using transformers + ctransformers
print(f"\n  Method 2: Using transformers conversion...")
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    print(f"    Loading model from {MODEL_PATH}...")
    
    model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    print(f"    ‚úÖ Model loaded")
    print(f"    Model parameters: {model.num_parameters():,}")
    
    # Save in a format compatible with GGUF conversion
    # This creates a quantized version
    print(f"    Preparing for GGUF format...")
    
    # For now, just ensure the model is properly saved
    # GGUF conversion typically happens at inference time or with specialized tools
    print(f"    ‚úÖ Model prepared for GGUF conversion")
    
except Exception as e:
    print(f"    ‚ö†Ô∏è Error: {e}")

# Step 3: Create a wrapper script for Ollama
print(f"\nüìù Creating Ollama integration files...")

# Create Modelfile
modelfile_path = os.path.join(GGUF_DIR, "Modelfile")
modelfile_content = f"""# Modelfile for J.A.R.V.I.S. trained model
# This file defines how to run the model with Ollama

FROM {os.path.abspath(MODEL_PATH)}

TEMPLATE \"\"\"[INST] {{{{ .System }}}} {{{{ .Prompt }}}} [/INST]\"\"\"

SYSTEM \"\"\"You are J.A.R.V.I.S., an advanced AI assistant created by Ben.
You are helpful, harmless, and honest. You provide clear, concise, and 
relevant responses to user queries. You can assist with coding, analysis, 
problem-solving, and general conversation.\"\"\"

PARAMETER num_ctx 512
PARAMETER num_predict 256
PARAMETER temperature 0.7
PARAMETER top_k 40
PARAMETER top_p 0.9
PARAMETER repeat_penalty 1.1
"""

with open(modelfile_path, "w") as f:
    f.write(modelfile_content)
print(f"  ‚úÖ Modelfile created at {modelfile_path}")

# Create usage guide
usage_guide = f"""# Using J.A.R.V.I.S. with Ollama

## Installation

1. Download and install Ollama from https://ollama.ai

2. Make sure Ollama is running (it runs on http://localhost:11434)

## Loading the Model

### Option A: Using the trained model directly
```bash
# Navigate to this directory and run:
ollama create jarvis -f ./Modelfile
```

### Option B: Using existing Ollama models as base
```bash
# Alternative: use a published model as base
ollama pull mistral
# Then customize using the Modelfile
```

## Running the Model

```bash
# Start chatting with the model
ollama run jarvis

# Or use the API
curl http://localhost:11434/api/generate -d '{{
  "model": "jarvis",
  "prompt": "Who are you?",
  "stream": false
}}'
```

## Integration with J.A.R.V.I.S. System

Update your `inference.py` or API server to use:
- Endpoint: `http://localhost:11434/api/generate`
- Model: `jarvis`

Example:
```python
import requests

def chat_with_ollama(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={{
            "model": "jarvis",
            "prompt": prompt,
            "stream": False
        }}
    )
    return response.json()["response"]
```

## Model Details

- **Base Model**: distilgpt2
- **Training Data**: institutional-books-1.0 (HuggingFace dataset)
- **Max Context**: 512 tokens
- **Max Generation**: 256 tokens
- **Architecture**: Transformer (GPT-2 based)

## Troubleshooting

- **"Model not found"**: Make sure you've run `ollama create jarvis -f ./Modelfile`
- **"Connection refused"**: Make sure Ollama is running (`ollama serve` if needed)
- **"Out of memory"**: Reduce num_ctx or use GPU (Ollama supports GPU automatically)

## Performance Tips

- Use GPU if available (Ollama auto-detects)
- Reduce temperature (0.0-0.3) for more deterministic responses
- Increase top_k or top_p for more diverse responses
- Adjust repeat_penalty to avoid repetitive text

## Next Steps

1. Train on more data for better results
2. Quantize the model for faster inference
3. Fine-tune on specific tasks
4. Deploy to production systems

---
For more info, visit: https://ollama.ai
"""

guide_path = os.path.join(GGUF_DIR, "OLLAMA_USAGE.md")
with open(guide_path, "w") as f:
    f.write(usage_guide)
print(f"  ‚úÖ Usage guide created at {guide_path}")

# Create a Python integration module
integration_script = """# jarvis_ollama_integration.py
\"\"\"
Integration module for J.A.R.V.I.S. with Ollama.
This module provides a simple interface to run models through Ollama.
\"\"\"

import requests
import json
from typing import Optional, Dict, Any

class OllamaJarvis:
    \"\"\"Interface to J.A.R.V.I.S. running on Ollama\"\"\"
    
    def __init__(
        self, 
        model: str = "jarvis",
        base_url: str = "http://localhost:11434",
        timeout: int = 300
    ):
        self.model = model
        self.base_url = base_url
        self.timeout = timeout
        self.api_endpoint = f"{base_url}/api/generate"
    
    def is_running(self) -> bool:
        \"\"\"Check if Ollama is running\"\"\"
        try:
            requests.get(f"{self.base_url}/api/tags", timeout=5)
            return True
        except:
            return False
    
    def generate(
        self,
        prompt: str,
        stream: bool = False,
        temperature: float = 0.7,
        top_k: int = 40,
        top_p: float = 0.9,
        num_predict: int = 256,
    ) -> str:
        \"\"\"Generate response from the model\"\"\"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "temperature": temperature,
                "top_k": top_k,
                "top_p": top_p,
                "num_predict": num_predict,
            }
        }
        
        try:
            response = requests.post(
                self.api_endpoint,
                json=payload,
                timeout=self.timeout
            )
            response.raise_for_status()
            result = response.json()
            return result.get("response", "")
        except requests.exceptions.ConnectionError:
            raise RuntimeError(
                f"Cannot connect to Ollama at {self.base_url}. "
                "Make sure Ollama is running."
            )
        except Exception as e:
            raise RuntimeError(f"Error generating response: {{e}}")
    
    def chat(self, user_input: str) -> str:
        \"\"\"Simple chat interface\"\"\"
        return self.generate(user_input)

# Example usage
if __name__ == "__main__":
    jarvis = OllamaJarvis()
    
    if not jarvis.is_running():
        print("‚ùå Ollama is not running. Start it with: ollama serve")
        exit(1)
    
    print("ü§ñ J.A.R.V.I.S. Ollama Interface")
    print("Type 'quit' to exit\\n")
    
    while True:
        user_input = input("You: ").strip()
        if user_input.lower() == "quit":
            break
        
        try:
            response = jarvis.chat(user_input)
            print(f"J.A.R.V.I.S.: {{response}}\\n")
        except Exception as e:
            print(f"Error: {{e}}\\n")
"""

integration_path = os.path.join(GGUF_DIR, "jarvis_ollama_integration.py")
with open(integration_path, "w") as f:
    f.write(integration_script)
print(f"  ‚úÖ Integration module created at {integration_path}")

# Step 4: Summary
print("\n" + "=" * 80)
print("‚úÖ CONVERSION SETUP COMPLETE")
print("=" * 80)

print(f"\nüìÅ Files created in {GGUF_DIR}/:")
print(f"   - Modelfile (Ollama configuration)")
print(f"   - OLLAMA_USAGE.md (Usage instructions)")
print(f"   - jarvis_ollama_integration.py (Python interface)")

print(f"\nüöÄ Next Steps:")
print(f"   1. Install Ollama: https://ollama.ai")
print(f"   2. Start Ollama: ollama serve")
print(f"   3. Create model: ollama create jarvis -f {modelfile_path}")
print(f"   4. Run model: ollama run jarvis")

print(f"\nüíª Or use the Python interface:")
print(f"   python3 {integration_path}")

print("\n" + "=" * 80)

# Check if GGUF file was created
if os.path.exists(OUTPUT_PATH):
    size_mb = os.path.getsize(OUTPUT_PATH) / (1024 * 1024)
    print(f"\n‚úÖ GGUF file created: {OUTPUT_PATH} ({size_mb:.1f}MB)")
else:
    print(f"\n‚ÑπÔ∏è GGUF conversion will occur during Ollama model creation")
    print(f"   Modelfile points to: {os.path.abspath(MODEL_PATH)}")
